{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndubuisijosephalx/Data-Science-projects/blob/main/nlp_Ndubuisi_J_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb16c732"
      },
      "outputs": [],
      "source": [
        "#!jupyter contrib nbextension install --user"
      ],
      "id": "cb16c732"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KWVShs_Ilpu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe81bc8-50b0-4182-afeb-e79d6eb6aad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "KWVShs_Ilpu6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqSKZB1TADlC"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "id": "PqSKZB1TADlC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASEDXWb3J0jf"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/python packages/"
      ],
      "id": "ASEDXWb3J0jf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzFvg67YGPfF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/python packages')\n",
        "#!pip install -r /content/drive/MyDrive/python packages/requirements.txt"
      ],
      "id": "KzFvg67YGPfF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "886edaa8"
      },
      "outputs": [],
      "source": [
        "#!pip install xgboost\n",
        "#!jupyter nbextension enable hinterland/hinterland"
      ],
      "id": "886edaa8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcbt3Ug7fQrf"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-plot"
      ],
      "id": "Tcbt3Ug7fQrf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNhvCeIRigSc"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n",
        "!pip install langid\n",
        "#!pip install spacy"
      ],
      "id": "NNhvCeIRigSc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82e3d017"
      },
      "source": [
        "# Import the Required Libraries for the Project Task"
      ],
      "id": "82e3d017"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e996486"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "import langid\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import spacy\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import string\n",
        "from langdetect import detect\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import scikitplot as skplt\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "import xgboost as xgb"
      ],
      "id": "7e996486"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a542ca56"
      },
      "outputs": [],
      "source": [
        "#Download NLTK resources (only needed if not downloaded previously)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "id": "a542ca56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "184778da"
      },
      "source": [
        "# Load the Corpus as Csv"
      ],
      "id": "184778da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a89b326"
      },
      "outputs": [],
      "source": [
        "# Set 'display.max_colwidth' to None to show the full content of all columns\n",
        "#pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Set 'display.max_rows' to None to show all rows without truncation\n",
        "#pd.set_option('display.max_rows', None)"
      ],
      "id": "4a89b326"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4d35815"
      },
      "outputs": [],
      "source": [
        "#resume_corpus = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\dataset\\Resume.csv\",sep=',')\n",
        "resume_corpus = pd.read_csv(\"/content/drive/MyDrive/dataset/Resume.csv\",sep=',')"
      ],
      "id": "a4d35815"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f6f4fd7"
      },
      "outputs": [],
      "source": [
        "resume_corpus.head()"
      ],
      "id": "1f6f4fd7"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "07c14465"
      },
      "source": [
        "resume_corpus.describe(include='All')"
      ],
      "id": "07c14465"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7d6215b"
      },
      "outputs": [],
      "source": [
        "resume_corpus.columns"
      ],
      "id": "d7d6215b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a0beb70"
      },
      "outputs": [],
      "source": [
        "resume_corpus.shape"
      ],
      "id": "4a0beb70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917e6ce4"
      },
      "source": [
        "# Class Percentage"
      ],
      "id": "917e6ce4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a83f9015"
      },
      "outputs": [],
      "source": [
        "# Compute the class percentage\n",
        "class_percentage = resume_corpus['Category'].value_counts()/len(resume_corpus)*100\n",
        "# diplay the result\n",
        "class_percentage"
      ],
      "id": "a83f9015"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34cad8b4"
      },
      "source": [
        "# Class Frequency"
      ],
      "id": "34cad8b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8a93ae9"
      },
      "outputs": [],
      "source": [
        "class_frequency = resume_corpus.Category.value_counts()\n",
        "class_frequency"
      ],
      "id": "a8a93ae9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b85eaa71"
      },
      "source": [
        "# Visualise the Class Frequency"
      ],
      "id": "b85eaa71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aced83e"
      },
      "outputs": [],
      "source": [
        "class_percentage.plot(kind = 'bar',rot=85)"
      ],
      "id": "4aced83e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ec7366"
      },
      "source": [
        "# Visualise the Class Distribution"
      ],
      "id": "10ec7366"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e4bd17b"
      },
      "outputs": [],
      "source": [
        "# Plotting the value counts of the 'Category' column as a bar plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(x='Category', data=resume_corpus)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Resumes per Category')\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()"
      ],
      "id": "3e4bd17b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJEI3SuLFDwT"
      },
      "source": [
        "##  Visualise the Class Frequency and Percentage"
      ],
      "id": "tJEI3SuLFDwT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOtiiBkCFDwU"
      },
      "outputs": [],
      "source": [
        "# Calculate class frequencies\n",
        "class_freq =  resume_corpus['Category'].value_counts()\n",
        "class_percentage = class_freq / len(resume_corpus) * 100\n",
        "\n",
        "# Plot the class frequency with percentage using seaborn barplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x=class_freq.index, y=class_freq.values)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Class Frequency')\n",
        "plt.ylim(top=max(class_freq.values) * 1.1)\n",
        "\n",
        "# Rotate x-axis tick labels by 70 degrees\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=70)\n",
        "\n",
        "# Add percentage labels on the bars\n",
        "for i, v in enumerate(class_percentage.values):\n",
        "    plt.text(i, v + 10, f'{v:.1f}%', ha='center',fontsize=6,color='white')"
      ],
      "id": "kOtiiBkCFDwU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4811eb7f"
      },
      "outputs": [],
      "source": [
        "category_counts = resume_corpus['Category'].value_counts()\n",
        "total_resumes = len(resume_corpus)\n",
        "category_percentages = (category_counts / total_resumes) * 100\n",
        "\n",
        "# Plotting the value counts of the 'Category' column as a count plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.countplot(x='Category', data=resume_corpus)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number and Percentage of Resumes per Category\\n')\n",
        "\n",
        "# Add percentage values on top of the bars\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2., height + 5, f'{height}\\n({height/total_resumes*100:.1f}%)',\n",
        "            ha='center', fontsize=6, color='black')\n",
        "\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()"
      ],
      "id": "4811eb7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bc81ea"
      },
      "source": [
        "# Check for Missing or Null Values"
      ],
      "id": "e8bc81ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f67eb60d"
      },
      "outputs": [],
      "source": [
        "resume_corpus.isna().sum()"
      ],
      "id": "f67eb60d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXOGv5sLVscd"
      },
      "source": [
        "## View How the class are distributed"
      ],
      "id": "OXOGv5sLVscd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMdMAvBCSmUD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot the scatterplot with the y-axis (count) on the x-axis and the x-axis labels on the y-axis\n",
        "sns.scatterplot(x=category_counts, y=resume_corpus['Category'].unique())\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Categories')\n",
        "plt.title('Category Counts Scatter Plot')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "uMdMAvBCSmUD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9d2b66c"
      },
      "source": [
        "# Examine the information about the corpus"
      ],
      "id": "d9d2b66c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "743de23b"
      },
      "outputs": [],
      "source": [
        "resume_corpus.info()"
      ],
      "id": "743de23b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "173f398d"
      },
      "outputs": [],
      "source": [
        "resume_corpus.Category.nunique()"
      ],
      "id": "173f398d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eef1bce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# View number of unique words\n",
        "resume_corpus.nunique()"
      ],
      "id": "5eef1bce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e179426"
      },
      "outputs": [],
      "source": [
        "resume_corpus.duplicated().sum()"
      ],
      "id": "6e179426"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdRTjgkqnLWX"
      },
      "outputs": [],
      "source": [
        "# View unique strings\n",
        "resume_corpus.Resume_str.unique()"
      ],
      "id": "qdRTjgkqnLWX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhCNOQzBYg2q"
      },
      "source": [
        "# Feature Engineering and Exploratory Data Analysis"
      ],
      "id": "UhCNOQzBYg2q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kD9iavkklFR"
      },
      "source": [
        "## Identify Languages in each row of Resume the Resume Column"
      ],
      "id": "_kD9iavkklFR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be58rGUOhXNY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Language Identification\n",
        "def identify_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "resume_corpus['Language'] = resume_corpus['Resume_str'].apply(identify_language)\n",
        "\n",
        "resume_corpus"
      ],
      "id": "be58rGUOhXNY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxDxUOG9s7PL"
      },
      "source": [
        "## Identify non-english language in the resume rows"
      ],
      "id": "JxDxUOG9s7PL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YF1mAiTl8Po"
      },
      "outputs": [],
      "source": [
        "resume_corpus[resume_corpus['Language']!='en']"
      ],
      "id": "8YF1mAiTl8Po"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AN79Mzure6E"
      },
      "source": [
        "## Retrieve the Length and Number of Puctuation in each row in the Resume column"
      ],
      "id": "2AN79Mzure6E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRdicZr3YgKs"
      },
      "outputs": [],
      "source": [
        "#resume_corpus = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create a function to count punctuation characters and length\n",
        "def count_punctuations_and_length(text):\n",
        "    num_punctuations = sum(1 for char in text if char in string.punctuation)\n",
        "    text_length = len(text)\n",
        "    return pd.Series([num_punctuations, text_length])\n",
        "\n",
        "# Step 2: Apply the function to the 'Resume_str' column and assign to new columns\n",
        "resume_corpus[['Punctuation_Count', 'Text_Length']] = resume_corpus['Resume_str'].apply(count_punctuations_and_length)\n",
        "# Step 3: Display the updated DataFrame\n",
        "display(resume_corpus.iloc[:10][:8])"
      ],
      "id": "xRdicZr3YgKs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb1ONHBGuISJ"
      },
      "source": [
        "## Count the number of words in each resume rows"
      ],
      "id": "eb1ONHBGuISJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA2fRy-cfCcL"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a CountVectorizer to calculate word frequencies\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform the 'Resume_str' column to get word frequencies\n",
        "word_frequencies = count_vectorizer.fit_transform(resume_corpus['Resume_str'])\n",
        "\n",
        "# Step 3: Get the feature names (words) from the CountVectorizer\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Step 4: Create a DataFrame for word frequencies and add to the original DataFrame\n",
        "word_freq_df = pd.DataFrame(word_frequencies.toarray(), columns=[f\"{word}_freq\" for word in feature_names])\n",
        "resume_corpus = pd.concat([resume_corpus, word_freq_df], axis=1)\n",
        "\n",
        "# Step 5: Display the updated DataFrame\n",
        "display(resume_corpus)"
      ],
      "id": "OA2fRy-cfCcL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcHeH_4kFDxA"
      },
      "source": [
        "## Extract features for machine learning"
      ],
      "id": "GcHeH_4kFDxA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlXStpoi4E4L"
      },
      "outputs": [],
      "source": [
        "resume_data = resume_corpus[['Punctuation_Count' ,'Text_Length']]\n",
        "resume_data"
      ],
      "id": "RlXStpoi4E4L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjW1q3wxFDxC"
      },
      "outputs": [],
      "source": [
        "resume_class = resume_corpus.Category\n",
        "resume_class.unique()"
      ],
      "id": "tjW1q3wxFDxC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhvu6S_2FDxD"
      },
      "outputs": [],
      "source": [
        "# view features description\n",
        "resume_data.describe()"
      ],
      "id": "Vhvu6S_2FDxD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh60S2E4FDxE"
      },
      "outputs": [],
      "source": [
        "resume_data.isna().sum()"
      ],
      "id": "Sh60S2E4FDxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj3IXkrhFDxF"
      },
      "source": [
        "The feature contains extreme values(oultiers), notice the difference between the mean and the max values"
      ],
      "id": "Yj3IXkrhFDxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naw0HI94FDxG"
      },
      "source": [
        "# Visualise the data with boxplot"
      ],
      "id": "naw0HI94FDxG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjL1egvtFDxG"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(resume_data)"
      ],
      "id": "OjL1egvtFDxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD_qbYOXFDxI"
      },
      "source": [
        "Notice the ouliers expecially in the text_length"
      ],
      "id": "vD_qbYOXFDxI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQ2lX4oFDxI"
      },
      "source": [
        "## Visualise the feature distribution"
      ],
      "id": "xDQ2lX4oFDxI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL2ubIR8FDxJ"
      },
      "outputs": [],
      "source": [
        "sns.displot(resume_data)"
      ],
      "id": "HL2ubIR8FDxJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGI-6u7zFDxK"
      },
      "source": [
        "the text_length seems to be almost righly skewed"
      ],
      "id": "NGI-6u7zFDxK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU8rnDv7FDxL"
      },
      "source": [
        "## Convert the class label to numerical values"
      ],
      "id": "tU8rnDv7FDxL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d780606"
      },
      "outputs": [],
      "source": [
        "# Create a mapping dictionary for the 'class' column\n",
        "class_mapping = {category: i for i, category in enumerate(resume_corpus['Category'].unique())}\n",
        "\n",
        "# Map the 'class' column to numerical values\n",
        "resume_corpus['label'] = resume_corpus['Category'].map(class_mapping)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "display(resume_corpus['label'].head())"
      ],
      "id": "9d780606"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYaWL7bZFDxN"
      },
      "source": [
        "# X-Boost Model without applying any vectorization matrix on our test"
      ],
      "id": "GYaWL7bZFDxN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LyBIsUtFDxO"
      },
      "outputs": [],
      "source": [
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(resume_data, resume_corpus['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Initialize the XGBoost classifier\n",
        "xclassifier = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',  # For multi-class classification\n",
        "    num_class=24,  # Number of classes\n",
        "    booster='gbtree',  # Tree-based models\n",
        "    max_depth=5,  # Maximum depth of the individual trees (you can tune this hyperparameter)\n",
        "    learning_rate=0.1,  # Learning rate (you can tune this hyperparameter)\n",
        "    n_estimators=100  # Number of boosting rounds (you can tune this hyperparameter)\n",
        ")\n",
        "\n",
        "# Step 5: Train the classifier on the training data\n",
        "xclassifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test data\n",
        "y_pred = xclassifier.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report for detailed performance metrics\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "id": "9LyBIsUtFDxO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzCWipDFDxP"
      },
      "source": [
        "Poorly performed model, just 6% accuracy. So let go ahead and do the real data cleaning, text processing, etc."
      ],
      "id": "omzCWipDFDxP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e4dabb8"
      },
      "source": [
        "# Data Cleaning"
      ],
      "id": "8e4dabb8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93d97d64"
      },
      "outputs": [],
      "source": [
        "# view the columns\n",
        "resume_corpus.columns"
      ],
      "id": "93d97d64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "145fb66d"
      },
      "outputs": [],
      "source": [
        "#  Select the important columns\n",
        "resume_corpus_refined = resume_corpus[['Resume_str','Category']].copy()\n",
        "resume_corpus_refined.head()"
      ],
      "id": "145fb66d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7be3b64"
      },
      "source": [
        "## Rename the Category to class for clarity"
      ],
      "id": "e7be3b64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXTgIWENy9YY"
      },
      "outputs": [],
      "source": [
        "# Renaming the 'Category' column to 'class'\n",
        "resume_corpus_refined.rename(columns={'Resume_str': 'resume', 'Category': 'class'}, inplace=True)\n",
        "#resume_corpus_refined = resume_corpus_refined.rename(columns={'Resume_str': 'resume', 'Category': 'class'})"
      ],
      "id": "hXTgIWENy9YY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXUmhGlTFDxU"
      },
      "outputs": [],
      "source": [
        "resume_corpus_refined.head()"
      ],
      "id": "AXUmhGlTFDxU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ffce2d"
      },
      "source": [
        "# Convert the class strings to numerical labels"
      ],
      "id": "84ffce2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48689f96"
      },
      "outputs": [],
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'class' column to numerical labels\n",
        "resume_corpus_refined['class_numerical'] = label_encoder.fit_transform(resume_corpus_refined['class'])\n",
        "\n",
        "# Display the updated DataFrame\n",
        "display(resume_corpus_refined.head())"
      ],
      "id": "48689f96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c871a0e"
      },
      "outputs": [],
      "source": [
        "# Get the unique values of the 'class' column\n",
        "unique_class_values = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Get the unique values of the 'class_numerical' column\n",
        "unique_class_numerical_values = resume_corpus_refined['class_numerical'].unique()\n",
        "\n",
        "# Create a dictionary matching unique class values with numerical class values\n",
        "class_mapping_dict = {class_value: class_numerical_value for class_value, class_numerical_value in zip(unique_class_values, unique_class_numerical_values)}\n",
        "\n",
        "# Print the unique values and their corresponding numerical values together\n",
        "print(\"Unique values and their corresponding numerical values:\")\n",
        "for class_value, class_numerical_value in class_mapping_dict.items():\n",
        "    print(f\"'{class_value}' -> {class_numerical_value}\")"
      ],
      "id": "5c871a0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmBPszSic9j"
      },
      "source": [
        "## Get the Class Labels"
      ],
      "id": "uSmBPszSic9j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d736373"
      },
      "outputs": [],
      "source": [
        "resume_corpus_refined['class'].unique()"
      ],
      "id": "5d736373"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4cdd8c0"
      },
      "source": [
        "# Text Preprocessing"
      ],
      "id": "f4cdd8c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a21b94"
      },
      "source": [
        "## Method 1: Preprocess Text with nltk"
      ],
      "id": "02a21b94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a6ff95d"
      },
      "outputs": [],
      "source": [
        "#nltk.download('omw')"
      ],
      "id": "0a6ff95d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4df68d0f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "id": "4df68d0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81541881"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing function\n",
        "def preprocess_text_nltk(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove special characters and numbers\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Perform stemming or lemmatization (choose one)\n",
        "    porter = PorterStemmer()\n",
        "    tokens = [porter.stem(token) for token in tokens]\n",
        "    #lemmatizer = WordNetLemmatizer()\n",
        "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to 'resume' column\n",
        "resume_corpus_refined['Cleaned_Resume'] = resume_corpus_refined['resume'].apply(preprocess_text_nltk)\n",
        "\n",
        "# Now cv_data['Cleaned_Resume'] column contains the preprocessed text data\n",
        "\n",
        "# Print a sample preprocessed resume\n",
        "print(resume_corpus_refined['Cleaned_Resume'].iloc[0])"
      ],
      "id": "81541881"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b17cff2"
      },
      "source": [
        "## Vectorize Text with TFIDF"
      ],
      "id": "5b17cff2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42a8422e"
      },
      "outputs": [],
      "source": [
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the 'Cleaned_Resume' text data into TF-IDF vectors\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(resume_corpus_refined['Cleaned_Resume'])\n",
        "\n",
        "# Convert the TF-IDF vectors to a DataFrame for better readability (optional)\n",
        "tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Now you have the preprocessed text data represented as TF-IDF vectors in 'tfidf_vectors'\n",
        "# The 'tfidf_df' DataFrame contains the same information but in a more readable format\n",
        "\n",
        "# Print the first few rows of the 'tfidf_df' DataFrame\n",
        "display(tfidf_df.head())"
      ],
      "id": "42a8422e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFx1dCUKsDM"
      },
      "source": [
        "## Display non-zero vectors"
      ],
      "id": "4JFx1dCUKsDM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJ5Ae_fLl_F",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the number of non-zero values for each feature\n",
        "num_non_zero_values = (tfidf_df > 0).sum()\n",
        "\n",
        "# Step 2: Repeat the feature names based on the number of non-zero values for each feature\n",
        "feature_names_repeated = tfidf_df.columns.repeat(num_non_zero_values)\n",
        "\n",
        "# Step 3: Repeat the non-zero values for each feature\n",
        "non_zero_values_repeated = tfidf_df.values[tfidf_df > 0]\n",
        "\n",
        "# Step 4: Create a new DataFrame with the non-zero TF-IDF scores and their corresponding feature names\n",
        "non_zero_tfidf_df = pd.DataFrame({'TF-IDF Score': non_zero_values_repeated}, index=feature_names_repeated)\n",
        "\n",
        "# Display the DataFrame with non-zero TF-IDF scores and their feature names\n",
        "display(non_zero_tfidf_df)"
      ],
      "id": "XLJ5Ae_fLl_F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923bcb30"
      },
      "source": [
        "# Build the First Baseline Model with our TFIDF Vector"
      ],
      "id": "923bcb30"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "688baf04"
      },
      "source": [
        "## Model 1: Logistic Regression Classifier"
      ],
      "id": "688baf04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8f40b59"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, resume_corpus_refined['class_numerical'], test_size=0.2, random_state=0,stratify = resume_corpus_refined['class_numerical'])\n",
        "\n",
        "# Initialize the logistic regression classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "id": "e8f40b59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Qn5PtC1hTx"
      },
      "source": [
        "## Compute and plot the confusion matrix for the baseline model(Model 1)"
      ],
      "id": "-9Qn5PtC1hTx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5231538d"
      },
      "outputs": [],
      "source": [
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels\n",
        "plt.figure(figsize=(18, 16))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18, 16))\n",
        "sns.heatmap(cm_percentage, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix (Percentages)')\n",
        "plt.show()"
      ],
      "id": "5231538d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aadf11a"
      },
      "source": [
        "## Model 2 :Support Vector Machine Classifier with ITIDF Vectors"
      ],
      "id": "9aadf11a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9b84b1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the Support Vector Machine (SVM) classifier\n",
        "# For binary classification, use SVC(kernel='linear')\n",
        "# For multiclass classification, use SVC(kernel='linear', decision_function_shape='ovr') or SVC(kernel='linear', decision_function_shape='ovo')\n",
        "svm_classifier = SVC(kernel='linear', decision_function_shape='ovr')\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_2 = svm_classifier.predict(X_test)\n",
        "y_pred_2"
      ],
      "id": "a9b84b1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrWYjg_qZxnN"
      },
      "source": [
        "## Compute and Plot the Confusion Matrix"
      ],
      "id": "MrWYjg_qZxnN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xPJ4Xo0Z6QF"
      },
      "outputs": [],
      "source": [
        "# Create a confusion matrix\n",
        "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_2 = cm_2.astype('float') / cm_2.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm_percentage, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix (Percentages)')\n",
        "plt.show()"
      ],
      "id": "6xPJ4Xo0Z6QF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5740b345"
      },
      "source": [
        "##  Evaluate the Model Performance"
      ],
      "id": "5740b345"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b7d597c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model's performance\n",
        "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "report_2 = classification_report(y_test, y_pred_2)\n",
        "print(\"Classification Report:\\n\", report_2)"
      ],
      "id": "7b7d597c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "609e0b70"
      },
      "source": [
        "## Model 3: Random Forest Model"
      ],
      "id": "609e0b70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a510c423"
      },
      "outputs": [],
      "source": [
        "\n",
        "# plit the data into training and testing sets\n",
        "#X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(tfidf_vectors, resume_corpus_refined['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest classifier with different variable names\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier on the training data\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_rf = random_forest_classifier.predict(X_test)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_rf = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels_rf = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels for Random Forest model\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels_rf, yticklabels=class_labels_rf)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Random Forest Confusion Matrix (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm_percentage_rf, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels_rf, yticklabels=class_labels_rf)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Random Forest Confusion Matrix (Percentages)')\n",
        "plt.show()"
      ],
      "id": "a510c423"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7534313e"
      },
      "source": [
        "## Evaluate the Model Performance"
      ],
      "id": "7534313e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03c136ff",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Evaluate the Random Forest model's performance\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "print(\"Random Forest Classification Report:\\n\", report_rf)"
      ],
      "id": "03c136ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "270d49a4"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download en_core_web_lg"
      ],
      "id": "270d49a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485ff579"
      },
      "source": [
        "# Preprocess Text with Spacy"
      ],
      "id": "485ff579"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1b37bc"
      },
      "source": [
        "## Examing the First Resume Content"
      ],
      "id": "6c1b37bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbee0dd8"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(resume_corpus_refined['resume'][0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_,token.is_oov,token.has_vector,token.is_stop,token.vocab)"
      ],
      "id": "dbee0dd8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA32WLfN3OPc"
      },
      "outputs": [],
      "source": [
        "doc.has_vector"
      ],
      "id": "yA32WLfN3OPc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7bb2723"
      },
      "source": [
        "## Process the Text"
      ],
      "id": "a7bb2723"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57787925"
      },
      "outputs": [],
      "source": [
        "# Load the English language model for spaCy\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a function to preprocess text using spaCy\n",
        "#def preprocess_text_spacy(text):\n",
        "    # Convert text to lowercase\n",
        " #   text = text.lower()\n",
        "#\n",
        " #   # Remove special characters and punctuation using spaCy tokenizer\n",
        "  #  doc = nlp(text)\n",
        "   # words = [token.text for token in doc if token.is_alpha]\n",
        "\n",
        "    # Remove stopwords using spaCy\n",
        "    #words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "    # Lemmatize the words using spaCy\n",
        "    #words = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Join the processed words back into a single string\n",
        "    #processed_text = ' '.join(words)\n",
        "\n",
        "    #return processed_text\n",
        "\n",
        "# Assuming 'resume_corpus_refined' is a list of resume texts\n",
        "#preprocessed_resumes_spacy = [preprocess_text_spacy(resume) for resume in resume_corpus_refined['resume']]"
      ],
      "id": "57787925"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVVRStTv1rlb"
      },
      "outputs": [],
      "source": [
        "# Load the English language model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "# Define a function to preprocess text and obtain word embeddings using spaCy\n",
        "def preprocess_text_and_get_embeddings(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation using spaCy tokenizer\n",
        "    doc = nlp(text)\n",
        "    words = [token.text for token in doc if token.is_alpha and not token.is_punct]\n",
        "    # Remove stopwords using spaCy\n",
        "    words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "    # Lemmatize the words using spaCy\n",
        "    words = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Join the processed words back into a single string\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    # Get the word embedding vector for the processed text\n",
        "    word_embedding_vector = doc.vector\n",
        "\n",
        "    return processed_text, word_embedding_vector\n",
        "\n",
        "# Assuming 'resume_corpus_refined' is a DataFrame with a column named 'resume'\n",
        "# You can replace 'resume_corpus_refined' with your actual DataFrame name\n",
        "# Create lists to store the processed text and word embedding vectors\n",
        "processed_texts = []\n",
        "word_embedding_vectors = []\n",
        "\n",
        "# Iterate through each resume and preprocess it using spaCy while obtaining word embeddings\n",
        "for index, row in resume_corpus_refined.iterrows():\n",
        "    processed_text, word_embedding_vector = preprocess_text_and_get_embeddings(row['resume'])\n",
        "    processed_texts.append(processed_text)\n",
        "    word_embedding_vectors.append(word_embedding_vector)\n",
        "\n",
        "# Create a new DataFrame from the collected lists\n",
        "processed_resumes_with_embeddings = pd.DataFrame({'processed_text': processed_texts,\n",
        "                                                  'word_embeddings': word_embedding_vectors})"
      ],
      "id": "hVVRStTv1rlb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CABwXc3R7U-0"
      },
      "outputs": [],
      "source": [
        "processed_resumes_with_embeddings"
      ],
      "id": "CABwXc3R7U-0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Logistic Classifier with Spacy MD"
      ],
      "metadata": {
        "id": "bdY-AOA9NdgX"
      },
      "id": "bdY-AOA9NdgX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (word embeddings) and labels (target)\n",
        "X = processed_resumes_with_embeddings['word_embeddings'].tolist()\n",
        "y = resume_corpus_refined['class_numerical']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify = resume_corpus_refined['class_numerical'])\n",
        "\n",
        "# Train the classifier on the scaled data\n",
        "classifier = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "classifier.fit(X_train, y_train)# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Get a detailed classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "BwRWDF9YNZh3"
      },
      "id": "BwRWDF9YNZh3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process text with spacy lg"
      ],
      "metadata": {
        "id": "fpR5x95foH43"
      },
      "id": "fpR5x95foH43"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the large English language model for spaCy\n",
        "nlp_large = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Define a function to preprocess text and obtain word embeddings using spaCy\n",
        "def preprocess_text_and_get_embeddings_large(text):\n",
        "    # Convert text to lowercase\n",
        "    processed_text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation using spaCy tokenizer\n",
        "    doc = nlp_large(processed_text)\n",
        "    words = [token.text for token in doc if token.is_alpha and not token.is_punct]\n",
        "\n",
        "    # Remove stopwords using spaCy\n",
        "    words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "    # Lemmatize the words using spaCy\n",
        "    words = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Join the processed words back into a single string\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    # Get the word embedding vector for the processed text\n",
        "    word_embedding_vector = doc.vector\n",
        "\n",
        "    return processed_text, word_embedding_vector\n",
        "\n",
        "# Assuming 'resume_corpus_refined' is a DataFrame with a column named 'resume'\n",
        "# You can replace 'resume_corpus_refined' with your actual DataFrame name\n",
        "# Create lists to store the processed text and word embedding vectors\n",
        "processed_texts_list = []\n",
        "word_embedding_vectors_list = []\n",
        "\n",
        "# Iterate through each resume and preprocess it using spaCy while obtaining word embeddings\n",
        "for index, row in resume_corpus_refined.iterrows():\n",
        "    processed_text, word_embedding_vector = preprocess_text_and_get_embeddings_large(row['resume'])\n",
        "    processed_texts_list.append(processed_text)\n",
        "    word_embedding_vectors_list.append(word_embedding_vector)\n",
        "\n",
        "# Create a new DataFrame from the collected lists\n",
        "processed_resumes_with_embeddings_lg = pd.DataFrame({'processed_text': processed_texts_list,\n",
        "                                                  'word_embeddings': word_embedding_vectors_list}"
      ],
      "metadata": {
        "id": "4HGARd_Kocnq"
      },
      "id": "4HGARd_Kocnq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model4: spacy large"
      ],
      "metadata": {
        "id": "vtI7CDuQq_TG"
      },
      "id": "vtI7CDuQq_TG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (word embeddings) and labels (target)\n",
        "X = processed_resumes_with_embeddings_lg['word_embeddings'].tolist()\n",
        "y = resume_corpus_refined['class_numerical']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify = resume_corpus_refined['class_numerical'])\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "classifier = SVC()\n",
        "\n",
        "# Train the SVM classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "p\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Get a detailed classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "BObG21z2TtNd"
      },
      "id": "BObG21z2TtNd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHwom1Sx7s5h"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download en_core_web_lg"
      ],
      "id": "AHwom1Sx7s5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d4616ac"
      },
      "outputs": [],
      "source": [
        "# display preprocessed text in a dataframe\n",
        "resume_corpus_refined['cleaned_resume_spacy_sm'] = preprocessed_resumes_spacy\n",
        "resume_corpus_refined"
      ],
      "id": "5d4616ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9SkRKMWxASW"
      },
      "outputs": [],
      "source": [
        "# Define a function to obtain word embeddings for a given resume text\n",
        "def get_word_embeddings(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.vector\n",
        "\n",
        "# Apply the function to get word embeddings for all resume texts\n",
        "resume_corpus_refined['word_embeddings'] = resume_corpus_refined['cleaned_resume_spacy_sm'].apply(get_word_embeddings)\n",
        "\n",
        "# Convert the word embeddings into separate columns in the DataFrame\n",
        "word_embeddings_df = pd.DataFrame(resume_corpus_refined['word_embeddings'].to_list())\n",
        "\n",
        "# Concatenate the original DataFrame with the word embeddings DataFrame\n",
        "resume_corpus_with_word_embeddings = pd.concat([resume_corpus_refined, word_embeddings_df], axis=1)"
      ],
      "id": "y9SkRKMWxASW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AyXuwzTyyxg"
      },
      "outputs": [],
      "source": [
        "\n",
        "    # embedding vector\n",
        "    resume_corpus_with_word_embeddings"
      ],
      "id": "2AyXuwzTyyxg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR0KcKdYFDx6"
      },
      "outputs": [],
      "source": [
        "# Assuming 'resume_corpus_refined' is a dictionary with 'cleaned_resume' as the key\n",
        "resumes = resume_corpus_refined['cleaned_resume_spacy_sm']\n",
        "\n",
        "# Create an empty list to store the vectors for each resume\n",
        "resume_vectors = []\n",
        "\n",
        "# Loop through each resume text and vectorize it using spaCy\n",
        "for resume_text in resumes:\n",
        "    doc = nlp(resume_text)\n",
        "    resume_vectors.append(doc.vector)"
      ],
      "id": "lR0KcKdYFDx6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7288c9c"
      },
      "outputs": [],
      "source": [
        "# Capture out-of-vocabulary (OOV) words\n",
        "oov_words = set()\n",
        "doc = nlp(resume_corpus_refined['cleaned_resume_spacy_sm'].to_string())\n",
        "for token in doc:\n",
        "    if not token.has_vector:  # Check if the word has a vector representation (i.e., recognized by the model)\n",
        "        oov_words.add(token.text)\n",
        "\n",
        "print(\"Out-of-Vocabulary (OOV) Words:\", oov_words)"
      ],
      "id": "a7288c9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76XuQDXVomPZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming 'resume_corpus_refined' is a list of cleaned resume texts as strings\n",
        "# Process each resume text to create spaCy Doc objects\n",
        "resumes = [nlp(resume_text) for resume_text in resume_corpus_refined['cleaned_resume_spacy_sm']]\n",
        "\n",
        "# Get individual sentences from each resume\n",
        "sentences = [list(resume.sents) for resume in resume_corpus_refined['cleaned_resume_spacy_sm']].to_list()\n",
        "\n",
        "# Flatten the list of sentences to form a list of words\n",
        "words = [str(word) for sentence in sentences for word in sentence]\n",
        "\n",
        "# Train Word2Vec model\n",
        "embedding_size = 100  # You can adjust this to the desired embedding size\n",
        "model = Word2Vec(sentences=words, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model or use it for further analysis\n",
        "model.save(\"word2vec_model.bin\")"
      ],
      "id": "76XuQDXVomPZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k30cHYq8qglw"
      },
      "outputs": [],
      "source": [
        "resume_corpus_refined['cleaned_resume_spacy_sm'].to_list()"
      ],
      "id": "k30cHYq8qglw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpk8qjIPtsL2"
      },
      "outputs": [],
      "source": [
        "model"
      ],
      "id": "tpk8qjIPtsL2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6267TjWTmKBG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load(\"word2vec_model.bin\")\n",
        "\n",
        "# Get word vectors for some sample words\n",
        "#sample_words = [\"resume\", \"skills\", \"experience\", \"job\", \"education\"]\n",
        "word_vectors = [model.wv[word] for word in resume_corpus_refined['cleaned_resume_spacy_sm']]\n",
        "\n",
        "# Find most similar words to a given word\n",
        "similar_words = model.wv.most_similar(resume_corpus_refined['cleaned_resume_spacy_sm'])\n",
        "\n",
        "# Example sentences]\n",
        "example_sentences = [\n",
        "    \"I have strong communication skills and extensive experience in project management.\",\n",
        "    \"My job responsibilities include analyzing data and creating insightful reports.\",\n",
        "    \"Education plays a crucial role in shaping one's career.\",\n",
        "    \"The candidate's resume was impressive, showcasing relevant skills and achievements.\",\n",
        "]\n",
        "\n",
        "print(\"Sample Word Vectors:\")\n",
        "for word, vector in zip(sample_words, word_vectors):\n",
        "    print(f\"{word}: {vector}\")\n",
        "\n",
        "print(\"\\nMost Similar Words to 'resume':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity:.4f}\")\n",
        "\n",
        "print(\"\\nExample Sentences:\")\n",
        "for sentence in example_sentences:\n",
        "    print(sentence)"
      ],
      "id": "6267TjWTmKBG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFo_w9sQlBgZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming 'resume_corpus_refined' is a list of cleaned resume texts as strings\n",
        "# Process each resume text to create spaCy Doc objects\n",
        "resumes = [nlp(resume_text) for resume_text in resume_corpus_refined['cleaned_resume_spacy_sm']]\n",
        "\n",
        "# Get individual sentences from each resume\n",
        "sentences = [list(resume.sents) for resume in resumes]\n",
        "\n",
        "# Flatten the list of sentences to form a list of words\n",
        "words = [str(word) for sentence in sentences for word in sentence]\n",
        "\n",
        "# Train Word2Vec model\n",
        "embedding_size = 100  # You can adjust this to the desired embedding size\n",
        "model = Word2Vec(sentences=words, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model or use it for further analysis\n",
        "model.save(\"word2vec_model.bin\")"
      ],
      "id": "HFo_w9sQlBgZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Gp4av-lfvP"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "id": "35Gp4av-lfvP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8e6a47d"
      },
      "outputs": [],
      "source": [
        "cd C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\"
      ],
      "id": "b8e6a47d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d54e388"
      },
      "outputs": [],
      "source": [],
      "id": "8d54e388"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac47cf99"
      },
      "outputs": [],
      "source": [
        "path = r\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\en_core_web_lg-3.6.0-py3-none-any.whl\""
      ],
      "id": "ac47cf99"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "012533f2"
      },
      "outputs": [],
      "source": [
        "import wheel\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace \"path/to/en_core_web_lg-3.6.0-py3-none-any.whl\" with the actual path to your wheel file\n",
        "wheel_file_path = r\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\en_core_web_lg-3.6.0-py3-none-any.whl\"\n",
        "\n",
        "# Create a directory to extract the contents of the wheel file\n",
        "extracted_dir = r\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\"\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "# Extract the wheel file\n",
        "with zipfile.ZipFile(wheel_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)"
      ],
      "id": "012533f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed9b4601"
      },
      "outputs": [],
      "source": [
        "path = r\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\en_core_web_lg\\en_core_web_lg-3.6.0\""
      ],
      "id": "ed9b4601"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee55fe11"
      },
      "outputs": [],
      "source": [
        "nlp_lg = spacy.load(path)"
      ],
      "id": "ee55fe11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d32f91da"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the spaCy English language model \"en_core_web_lg\"\n",
        "#nlp_lg = spacy.load(r\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\en_core_web_lg-3.6.0-py3-none-any.whl\")\n",
        "\n",
        "# Assuming you have already loaded the data into a DataFrame named resume_corpus_refined\n",
        "# resume_corpus_refined = pd.read_csv(\"path_to_your_csv_file.csv\")\n",
        "\n",
        "# Text preprocessing function using spaCy with the \"en_core_web_lg\" model\n",
        "def preprocess_text_spacy_lg(text):\n",
        "    doc = nlp_lg(text)\n",
        "    # Your preprocessing code using spaCy with \"en_core_web_lg\" (e.g., lemmatization, removing stop words, etc.)\n",
        "    processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing using spaCy to 'Resume_str' column if not done earlier\n",
        "resume_corpus_refined['Cleaned_Resume_lg'] = resume_corpus_refined['Resume'].apply(preprocess_text_spacy_lg)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer for the \"en_core_web_lg\" processed text\n",
        "embedding_vectorizer_lg = [token.vector for token in doc]\n",
        "\n",
        "# Convert the list of embeddings to a numpy array\n",
        "embedding_vectors_lg = np.array(embedding_vectorizer_lg)\n",
        "# Split the data into training and testing sets\n",
        "X_train_lg, X_test_lg, y_train_lg, y_test_lg = train_test_split(embedding_vectors_lg, resume_corpus_refined['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier for \"en_core_web_lg\" processed text with different variable names\n",
        "naive_bayes_classifier_lg = MultinomialNB()\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier on the training data\n",
        "naive_bayes_classifier_lg.fit(X_train_lg, y_train_lg)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_lg = naive_bayes_classifier_lg.predict(X_test_lg)\n",
        "\n",
        "y_pred_lg[:10]"
      ],
      "id": "d32f91da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b51c14e"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing function using spaCy with the \"en_core_web_lg\" model\n",
        "def preprocess_text_spacy_lg(text):\n",
        "    doc = nlp_lg(text)\n",
        "    # Your preprocessing code using spaCy with \"en_core_web_lg\" (e.g., lemmatization, removing stop words, etc.)\n",
        "    processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing using spaCy to 'Resume_str' column if not done earlier\n",
        "resume_corpus_refined['Cleaned_Resume_lg'] = resume_corpus_refined['Resume'].apply(preprocess_text_spacy_lg)\n",
        "\n",
        "# Create a vector representation for each document using the embeddings of \"en_core_web_lg\"\n",
        "embedding_vectors_lg = np.array([doc.vector for doc in nlp_lg.pipe(resume_corpus_refined['Cleaned_Resume_lg'])])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_lg, X_test_lg, y_train_lg, y_test_lg = train_test_split(embedding_vectors_lg, resume_corpus_refined['Category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier for \"en_core_web_lg\" processed text with different variable names\n",
        "naive_bayes_classifier_lg = MultinomialNB()\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier on the training data\n",
        "naive_bayes_classifier_lg.fit(X_train_lg, y_train_lg)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_lg = naive_bayes_classifier_lg.predict(X_test_lg)\n",
        "\n",
        "# View the first 10 predictions\n",
        "print(y_pred_lg[:10])"
      ],
      "id": "8b51c14e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31d12720"
      },
      "source": [
        "# Plot the Confusion Matrix"
      ],
      "id": "31d12720"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "817206c1"
      },
      "outputs": [],
      "source": [
        "# Create a confusion matrix\n",
        "cm_lg = confusion_matrix(y_test_lg, y_pred_lg)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_lg = cm_lg.astype('float') / cm_lg.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels_lg = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels for the Multinomial Naive Bayes model with \"en_core_web_lg\"\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_lg, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels_lg, yticklabels=class_labels_lg)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Multinomial Naive Bayes Confusion Matrix with en_core_web_lg (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_percentage_lg, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels_lg, yticklabels=class_labels_lg)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Multinomial Naive Bayes Confusion Matrix with en_core_web_lg (Percentages)')\n",
        "plt.show()"
      ],
      "id": "817206c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6469ab"
      },
      "source": [
        "# Evaluate the Model Performance"
      ],
      "id": "ce6469ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cb0091d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the Multinomial Naive Bayes model's performance with \"en_core_web_lg\" processed text\n",
        "accuracy_lg = accuracy_score(y_test_lg, y_pred_lg)\n",
        "print(\"Multinomial Naive Bayes Accuracy with en_core_web_lg:\", accuracy_lg)\n",
        "\n",
        "report_lg = classification_report(y_test_lg, y_pred_lg)\n",
        "print(\"Multinomial Naive Bayes Classification Report with en_core_web_lg:\\n\", report_lg)"
      ],
      "id": "2cb0091d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ca4d04b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply preprocessing using spaCy to 'Resume_str' column if not done earlier\n",
        "resume_corpus_refined['cleaned_Resume'] = resume_corpus_refined['resume'].apply(preprocess_text_spacy)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer_spacy = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the 'Cleaned_Resume' text data into TF-IDF vectors\n",
        "tfidf_vectors_spacy = tfidf_vectorizer_spacy.fit_transform(resume_corpus_refined['cleaned_resume'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_spacy, X_test_spacy, y_train_spacy, y_test_spacy = train_test_split(tfidf_vectors_spacy, resume_corpus_refined['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression classifier with different variable names\n",
        "logistic_classifier_spacy = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression classifier on the training data\n",
        "logistic_classifier_spacy.fit(X_train_spacy, y_train_spacy)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_spacy = logistic_classifier_spacy.predict(X_test_spacy)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm_spacy = confusion_matrix(y_test_spacy, y_pred_spacy)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_spacy = cm_spacy.astype('float') / cm_spacy.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels_spacy = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels for the Logistic Regression model with spaCy\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_spacy, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels_spacy, yticklabels=class_labels_spacy)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Logistic Regression Confusion Matrix with spaCy (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_percentage_spacy, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels_spacy, yticklabels=class_labels_spacy)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Logistic Regression Confusion Matrix with spaCy (Percentages)')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the Logistic Regression model's performance with spaCy\n",
        "accuracy_spacy = accuracy_score(y_test_spacy, y_pred_spacy)\n",
        "print(\"Logistic Regression Accuracy with spaCy:\", accuracy_spacy)\n",
        "\n",
        "report_spacy = classification_report(y_test_spacy, y_pred_spacy)\n",
        "print(\"Logistic Regression Classification Report with spaCy:\\n\", report_spacy)"
      ],
      "id": "3ca4d04b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a994db9d"
      },
      "source": [
        "# Apply preprocessing using spaCy to 'Resume_str' column if not done earlier\n",
        "resume_corpus_refined['cleaned_resume'] = resume_corpus_refined['Resume'].apply(preprocess_text_spacy)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer_spacy = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the 'Cleaned_Resume' text data into TF-IDF vectors\n",
        "tfidf_vectors_spacy = tfidf_vectorizer_spacy.fit_transform(resume_corpus_refined['cleaned_resume'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_spacy, X_test_spacy, y_train_spacy, y_test_spacy = train_test_split(tfidf_vectors_spacy, resume_corpus_refined['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression classifier with different variable names\n",
        "logistic_classifier_spacy = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the Logistic Regression classifier on the training data\n",
        "logistic_classifier_spacy.fit(X_train_spacy, y_train_spacy)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_spacy = logistic_classifier_spacy.predict(X_test_spacy)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm_spacy = confusion_matrix(y_test_spacy, y_pred_spacy)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_spacy = cm_spacy.astype('float') / cm_spacy.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels_spacy = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels for the Logistic Regression model with spaCy\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_spacy, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels_spacy, yticklabels=class_labels_spacy)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Logistic Regression Confusion Matrix with spaCy (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_percentage_spacy, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels_spacy, yticklabels=class_labels_spacy)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Logistic Regression Confusion Matrix with spaCy (Percentages)')\n",
        "plt.show()\n",
        "\n"
      ],
      "id": "a994db9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e717628f"
      },
      "source": [
        "## Evaluate the Model Performance"
      ],
      "id": "e717628f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe58c6e6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Logistic Regression model's performance with spaCy\n",
        "accuracy_spacy = accuracy_score(y_test_spacy, y_pred_spacy)\n",
        "print(\"Logistic Regression Accuracy with spaCy:\", accuracy_spacy)\n",
        "\n",
        "report_spacy = classification_report(y_test_spacy, y_pred_spacy)\n",
        "print(\"Logistic Regression Classification Report with spaCy:\\n\", report_spacy)"
      ],
      "id": "fe58c6e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e443560"
      },
      "outputs": [],
      "source": [],
      "id": "3e443560"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89e7c174"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the spaCy English language model \"en_core_web_lg\"\n",
        "nlp_lg = spacy.load(\"C:\\Users\\HP\\Desktop\\Msc Reinforcement learning\\en_core_web_lg-3.6.0-py3-none-any.whl\")\n",
        "\n",
        "# Assuming you have already loaded the data into a DataFrame named resume_corpus_refined\n",
        "# resume_corpus_refined = pd.read_csv(\"path_to_your_csv_file.csv\")\n",
        "\n",
        "# Text preprocessing function using spaCy with the \"en_core_web_lg\" model\n",
        "def preprocess_text_spacy_lg(text):\n",
        "    doc = nlp_lg(text)\n",
        "    # Your preprocessing code using spaCy with \"en_core_web_lg\" (e.g., lemmatization, removing stop words, etc.)\n",
        "    processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing using spaCy to 'Resume_str' column if not done earlier\n",
        "resume_corpus_refined['Cleaned_Resume_lg'] = resume_corpus_refined['Resume'].apply(preprocess_text_spacy_lg)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer for the \"en_core_web_lg\" processed text\n",
        "tfidf_vectorizer_lg = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the 'Cleaned_Resume_lg' text data into TF-IDF vectors\n",
        "tfidf_vectors_lg = tfidf_vectorizer_lg.fit_transform(resume_corpus_refined['Cleaned_Resume_lg'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_lg, X_test_lg, y_train_lg, y_test_lg = train_test_split(tfidf_vectors_lg, resume_corpus_refined['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting classifier for \"en_core_web_lg\" processed text with different variable names\n",
        "gradient_boosting_classifier_lg = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting classifier on the training data\n",
        "gradient_boosting_classifier_lg.fit(X_train_lg, y_train_lg)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_lg = gradient_boosting_classifier_lg.predict(X_test_lg)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm_lg = confusion_matrix(y_test_lg, y_pred_lg)\n",
        "\n",
        "# Calculate the percentages\n",
        "cm_percentage_lg = cm_lg.astype('float') / cm_lg.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Get the unique class labels\n",
        "class_labels_lg = resume_corpus_refined['class'].unique()\n",
        "\n",
        "# Plot the confusion matrix heatmap with custom labels for the Gradient Boosting model with \"en_core_web_lg\"\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_lg, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels_lg, yticklabels=class_labels_lg)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Gradient Boosting Confusion Matrix with en_core_web_lg (Numbers)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_percentage_lg, annot=True, cmap='Blues', cbar=False, fmt=\".2f\", xticklabels=class_labels_lg, yticklabels=class_labels_lg)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Gradient Boosting Confusion Matrix with en_core_web_lg (Percentages)')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the Gradient Boosting model's performance with \"en_core_web_lg\" processed text\n",
        "accuracy_lg = accuracy_score(y_test_lg, y_pred_lg)\n",
        "print(\"Gradient Boosting Accuracy with en_core_web_lg:\", accuracy_lg)\n",
        "\n",
        "report_lg = classification_report(y_test_lg, y_pred_lg)\n",
        "print(\"Gradient Boosting Classification Report with en_core_web_lg:\\n\", report_lg)"
      ],
      "id": "89e7c174"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}